---
title: "practical knowledge collection"
last_modified_at: 2022-12-31T16:20:02-05:00
categories:
  - deep-learning
---

학습 실행 전 체크 리스트
- abc



# 배치 단위로 가장 긴 길이에 맞춰서 padding을 줘서 학습 속도 향상시키기

- 전체 데이터셋에서 가장 긴 sequence 길이에 padding을 맞추면 메모리도 많이 들고 학습 속도도 느려진다. 가장 긴 sequence 만큼 길이가 긴 데이터가 없는 배치들도 있는데 다 가장 긴 곳에 padding 맞춰지기 때문
- 배치 마다 가장 긴 길이로 맞추면 많은 경우 아주 빨라짐!
- [DataCollatorWithPadding](
        https://huggingface.co/docs/transformers/main_classes/data_collator)
    - 입력으로 토크나이저를 거친 encoding object가 들어오면 배치 안에서 길이에 맞게 padding을 조절해줌
    - 따라서 toknizer으로 encoding object을 만들때 padding 없이 그냥 index으로 변환만 해준다.
    - dataloader에 collate_fn에 DataCollatorWithPadding을 적용해주면 됨.
    - DataCollatorWithPadding의 설정에 배치 내부에 적용할 padding 및 다른 tokenier 설정을 해준다. huggingface의 tokenizer의 argument을 그대로 쓰면 됨.
    - trainer api을 사용할 경우, Trainer을 생성할 때 data_collator의 값으로 DataCollatorWithPadding의 instance을 보내주면 알아서 dataloader와 연결해준다.

```python
from torch.utils.data import Dataset
class dataset(Dataset):

    def __init__(tokenizer, ...):
        self.tokenizer = tokenizer

    def __getitem__(self, index):
        label = self.label[index]
        content = self.content[index]

        # padding, truncation 등을 하지 않고 그냥 변환만 해준다.
        # collator을 사용해서 능동적으로 제어할 것
        features = self.tokenizer(
            content
        )   
        features['labels'] = label

        return features

```

```python
from transformers DataCollatorWithPadding
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from transformers import Trainer


tokenizer = AutoTokenizer.from_pretrained(model_name)


# 각 배치 마다 가장 긴 길이로 padding 하지만 512을 넘어가면 자른다.
# 입력은 encoding batch이고, argument 설정을 따라 padding 등을 수정해준다.



collator = transformers.DataCollatorWithPadding(tokenizer, padding=True, truncation = True, max_length=512, )    

# 내부에서 dataloader에 collate_fn에 집어넣은 collator을 설정해준다.
trainer = MyTrainer(
    ...,
    data_collator=collator,
    ...
)


```

## 데이터셋의 최대 토큰 길이 확인해서 적용하기

```python

from torch.utils.data import Dataset
class dataset(Dataset):

    def __init__(tokenizer, data, ...):
        self.tokenizer = tokenizer
        
        self.texts = self.data['texts'].tolist()

    def check_max_seq_length(self):
        # (batch, seq_len)
        return self.tokenizer(self.texts)['input_ids'].shape[1]    

   def __getitem__(self, index):
        label = self.label[index]
        content = self.content[index]

        # padding, truncation 등을 하지 않고 그냥 변환만 해준다.
        # collator을 사용해서 능동적으로 제어할 것
        features = self.tokenizer(
            content
        )   
        features['labels'] = label

        return features


from transformers DataCollatorWithPadding
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from transformers import Trainer


tokenizer = AutoTokenizer.from_pretrained(model_name)



dataset = ...


# 1. 가장 긴 길이로 padding을 다 맞춘다. 
# 2. 배치 크기 조정해서 얼마나 들어갈 수 있는지 확인

collator = transformers.DataCollatorWithPadding(tokenizer, padding=True, truncation = 'max_length', max_length=dataset.check_max_seq_length(), )    

# 3. 해당 크기로 배치 설정 후 다시 padding을 True으로 바꾼다. 그러면 가장 긴 데이터 길이가 있느 곳을 제외하고는 모두 배치 내부에서 최대 길이로 padding이 맞춰짐. 모든 배치를 다 가장 길게 하지 않아서 학습 속도 향상 됨.
# collator = transformers.DataCollatorWithPadding(tokenizer, padding=True, truncation = True)  

# 내부에서 dataloader에 collate_fn에 집어넣은 collator을 설정해준다.
trainer = MyTrainer(
    ...,
    data_collator=collator,
    ...
)        
```


# tokenizer 설정
[링크](https://huggingface.co/docs/transformers/pad_truncation
)
- `padding = True`: 각 배치 내부에서 가장 긴 길이에 padding을 맞춰준다. 
- `padding = max_length`: argument으로 들어온 max_length으로 padding을 맞춘다. 그래서 모든 배치 마다 max_length으로 맞춰짐. 
- `truncation = True`: argument으로 들어온 max_length 길이 보다 길면 자른다. max_lenght가 없으면 모델 최대 길이가 max_lemgth가 된다.
