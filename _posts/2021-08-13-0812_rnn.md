---
title: "0812_rnn"
last_modified_at: 2021-08-06T16:20:02-05:00
categories:
  - Blog
  - Camp
tags:
  - camp
---

# week 2 Thu 0812
## Seqeunce Data and RNN
* 일상의 대부분 seq 데이터.
* 원하는 것. 단순. 길이가 언제 끝날지 모름. 그래서 고정되어 있는 conv을 쓸 수 없다. 어느 길이까지 받아야 할지 모르니까.
* lang model: 이전 데이터로 다음 데이터를 예측하는 것. 갈 수로 고려해야하는 과거 정보가 많아진다. 
* 단순 생각: 과거 t 시점까지만 보자. 이게 markov model. 내가 가정 하기에 내 과거는 바로 전 과거에만 종속. 내일 수능 점수는 하루 전의 과거에만 의존. 그래서 많은 정보를 버린다. 대신에 조인트가 단순. 
* h는 과거 정보를 모두 요약. 그거랑 조인트! 간결!
* 하나의 가중치로 계속 학습. 아주 먼 과거의 데이터가 소실 됨. 아주 긴 문장의 앞 단어들을 놓침. short term dependency. 
* 계산식: 중첩된다. 값이 의미가 없어짐. 만약 activation이 sigmoid이고 작거나 크면라면 vanishing. relu라면 w가 1보다 클 때 explod.
* 해결 -> lstm. 

## LSTM
* 어떻게 long term dep을 해결했는가?
* input gate
  * 어떤 정보를 추가할지 결정
* forget gate
  * 어떤 정보를 잊을지 결정
* output gate
  * 어떤 정보를 다음 cell로 보낼지 결정

# transformer
* 인코더 디코더 모델에서 시작!
* no more sequential! parellel!!!
* attention + cnn
* parellel with cnn style

## key ideas:
* self attention. 
  * e.g. 5 words. attention in parellel. 
* multiple version

# self attention
$$
A(q, K, V)=\sum_{i} \underbrace{\frac{\exp \left(q \cdot k^{<i}\right)}{\sum_{j} \exp \left(q \cdot k^{<j>}\right)}} v^{<i>}
$$

## 트랜스포머 설명 바이 설명

https://nlpinkorean.github.io/illustrated-transformer/

* 번역 문제에 적용
1. 문장을 한꺼번에 다 집어넣는다. 병렬적으로.
2. 각 단어 마다 미리 설정해둔(pretrain이든 뭐든) 임베딩을 사용해서 dense word embeding 벡터들로 만든다.
3. 각 단어들 마다 파라미터 벡터 $W^q. W^k, W^v$을 내적시켜서 $q,k,v$ 벡터를 만든다. 
4. 각 단어의 q와 다른 모든 단어들의 k을 내적사켜서 단어들 사이에 의미의 유사도를 찾는다. (코사인 유사도?). 
5. 상수로 나눠서 값을 줄인다. 
6. 그리고 소프트맥스를 해서 유사도에 대한 가중치를 만든다. 
7. 그리고 각 단어의 가중치와 v을 곱해서 모두 더하면... 각 단어의 q와 유사한 단어들과의 value 가중 평균 값을 얻는다!
8. 이 과정을 병렬적으로 하기 위해서 행렬로 만든다. vectorization!

* 이러면 self attention이 끝. 이게 1개의 head이다.

* 이것을 서로 다른 8개를 실행 = multihead. 
* featuremap과 유사? 
* 서로 다른 hidden representation을 만드는 것.
* 각 head는 각 단어와의 유사성을 다르게 찾는다. 

* 생각해볼 질문: q,k,v을 왜? 어떻게 생각하게 되었을까? 디비와의 연관성은? 

* decoder에 넣는다.
* mask 사용ㅎ서 미래 쿼리 막는다
* 맨 처음 학습에 의한 기본 출력? startToken이 입력으로 들어온다.
* 그 출력이 입력 쿼리로 온다. 
* 인코더에서 받은 k,v 매트릭스와 multihead와 내적 weighted sum. 
* 합쳐서 벡터 출력. 
* normalize 하고 ff 거쳐서 softmax으로 단어 예측 하고 argmax으로 가장 큰 단어 vocab에서 추출.