---
title: "0811_cnn"
last_modified_at: 2021-08-11T16:20:02-05:00
categories:
  - boostcamp
tags:
  - camp
---
# week 2 Wed 0811
* convolution의 직관
  * 도장을 찍는다
  * 의미: 필터에 찍는다. 필터 모양에 따라 다른 결과가 나온다. 필터의 평균: 블러 등등
* 필터의 수만큼 결과의 채널
* 1번 cov 후 activication.  
  * e.g. relu
* 조심: 파라미터의 숫자 = 채널에 들어가는 칸의 수. 
  * f * f * n_c_prev * n_c

## 구성
* conv
* FC: classification. 파라미터 수를 줄이기 위해 최근 트렌드는 뒤를 conv으로 바꿈. 파라미터 수가 * 크면 일반화 성능이 떨어진다.
* stride: 건너 뛴다
* padding: 가장 자리에 0을 붙인다
* 레이어 별로 파라미터 수가 몇개인지 감을 항상 가지고 있어야 한다. 
* 출력의 feature map의 크기를 조정 가능
* stride = 1, padding = 1이면 입력과 같은 수가 나온다.
* 네트워크 모양만 봐도 파라미터 수가 큰지 작은지 감이 생겨야 한다. 만단위인지 십만인지 백만인지 등

## 특징
* Alex net을 보면 conv 단에서는 파라미터의 수가 K단위
* dense layer으로 가면 M단위로 바뀐다
* 왜? parameter sharing
* dense layer -> conv layer through 1 by 1 conv
* 1 x 1 conv 직관: h,w, n_c의 3차원 텐서. 1 1 conv chnnel 1개는 전체 n_c에 대해 1번 씩 * conv해서 더함. 내적고ㅏ 같은 의미. 이게 1개의 뉴런. n개의 ㅣ채널이 있으면 n개의뉴런. dense * layer와 같은 연산. 

* 과제 인사이트: cnn의 general performance가 좋더라
* 레이어에 이름을 주면 디버깅할 때 편하다

## moderan cnn
* 깊어지고 파라미터 작아지고 성능은 좋아진다
* ILSVRC: object detection 챌린지 
* alex net
  * gpu크기 때문에 두개로 나눔
  * 11 11 필터. 채널 수는 더 많아지게 되어서 결과적으로 비효율
  * 키 아이디어: relu 사용. GPU. data augmentation. drop out. 지금의 잘 되는 기준을 잡은 * 논문. sigmoid. 0을 기점으로 slope가 줄어든다. vanishing grad 발생. 
* VGG net
  * 33 필터 활용. 11 활용해서 fc을 conv으로 바꿈. drop out... why 33 filter? 커지는 이점: * 더 넓게 볼 수 있다. 33 두개 쓰는게 55 하나 쓰는 것과 결과는 같지만 파라미터 수는 훨씬 작아 짐. * parameter sharing. 인사이트: 55 보다 33이 좋다.
* 구글넷
  * 중간 중간에 11을 써서 더 줄임. 메모리 세이브. 비슷한 모양이 반복. 네트워크 인 네트워크. NIN.
  * inception block 안에 11을 넣어서 보틀넥. 파라미터 수를 엄청 줄인다 + 여러 correlation을 * 모은다. 먼저 11 필터를 사용해서 채널 수를 낮춘다. 11필터를 쓰면 크기는 유지되지만, conv * 레이어에서 output필터 수를 줄여서 전체 채널을 낮춘다. 추상적 정보를 집약시키는 것, 그리고 33,55을 * 적용.
* resnet
  * 문제: generalization performance. 가 안좋다의 두가지 경우
  * 1. overffiting: 에폭이 반복될 수록 train에러는 작다. test 에러의 크기가 커진다
  * 2. 에폭이 반복될 수록 train error도 감소를 안한다.
  * 해결: identity map추가. 
  * 차이만 새로 학습.
  * main path에 새로 학습한 내용 더한다
  * 11 필터를 conv 해서 채널 수를 맞춰 준다.
  * batch norm이 conv 뒤에 온다. 논란: relu 다음에 bn이 오기도 한다.
  * 전략: 33 55 필터로 spatial size줄이고 11 필터로 원하는 채널 만들면서 깊게 쌓는다
* DenseNet
  * resnet은 identity map을 더한다. densenet은 concatinate .
  * 근데 concat은 채널 수가 두배가 됨. 계속 두배가 되면 exp하게 채널 수가 커진다. 
  * 구현
  * transition block -> batch norm -> 11 conv(channel 사이즈 줄임) -> 22 avPool -> * transition block
* cnn으로 뭘 만들어야겠다! -> resnet 혹은 densenet을 써라.# application

## sementic segmentaion
* 픽셀 단위로 어느 object에 속하는지.
* u net
* 자율주행
* 라이더 없이
* FC을 conv으로 바꿈: convolutionalization. fulliy convolution network으로 바꾸는 것.
* 왜 이걸 하냐? yolo 예제 처럼... 
  * 겹치는 부분이 겹치는 부분이 사실 아니다. conv 특성을 따라 인접 영역까지 내적해서 새로운 칸에 저장. deconvolution = conv transpose
  * spation dimension을 키워준다. 근데 사실 conv에 역은 없다. 복원은 불가능. 

## Detection
* RCNN
  * pixel 단위가 아니라 바운딩박스로 classification. 이미지 안에서 패치를 뽑는다. 똑같은 크기로 cnn에 넣는다. 분류는 svl으로 함. linear support vector. 
  * 문제: 패치 마다 한번씩 cnn 돌려야.
* sppnet 
  * 1번만 돌자. 결과에서 패치 나누자. 비슷한 RCNN. 바운딩 박스 만들고. 1번 FF. 결과에서 각 박스 결과 확인 및 학습. Fater Rcnn은 바운딩 박스를 뽑는 기준도 학습하자이다. 중간에 RPN (region proposal network): 해당 영역에 물체가 있을지 학습. 바운딩 박스가 유의미한지, 위치와 크기를 어떻게 바꿀지 학습. 
* yolo
  * 바운당박스를 따로 뽑는 rpn이 없다. 이미지 넣으면 한번에 박스도 찾고 classification도 한꺼번에 한다. 