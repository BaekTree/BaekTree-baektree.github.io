---
title: "0806_math"
last_modified_at: 2021-08-06T16:20:02-05:00
categories:
    - boostcamp
tags:
  - math
  - camp
---

# CNN

It seems reasonable that whatever method we use to recognize objects should not be overly concerned with the precise location of the object in the image.

CNNs systematize this idea of spatial invariance, exploiting it to learn useful representations with fewer parameters.

우리가 사진에서 특정 물체를 찾을 때 눈으로 이곳 저곳을 살펴보게 됨. 기준을 떠올리면서 그 기준에 부합하는지 찾음. DNN 역시 비슷하게 작동해야 함. 학습으로 찾은 특정한 기준을 사진의 부분들에 적용해보면서 그 기준에 맞는지 찾는다. 맞으면 True 틀리면 False. Multi Class을 찾는 것이라면 어느 class에 해당하는지 확인하고 class을 특정해냄. 없으면 없다고 결정. 따라서 특정 시야에서 판단의 기준이 되는 파라미터를 학습시키면 된다. 그래서 파라미터의 수가 사진 크기에 비해서 비교적 적어도 됨. 같은 기준을 가지고 사진 영역 전체에 적용한다고 해서 spatial invariance라고 부른다. 각기 다른 영역에서 기준은 변하지 않는다. 


In the earliest layers, our network should respond similarly to the same patch, regardless of where it appears in the image. This principle is called translation invariance.

입력을 받아들이는 레이어에서 CNN은 사진의 각 영역(patch)에서 일정하게 반응해야 한다. 이미지 어디에서 나타나든 간에. 영역의 위치가 어디이든 간에 찾고자 하는 물체가 있으면 동일하게 판단해야 함. patch는 같은 기준을 가지고 있고, kernel 혹은 채널에 해당한다. 

맨 앞 레이어는 사진의 (local)부분에만 집중한다. 사진 전체가 아니라 부분. 이 부분들이 합쳐져서 전체 이미지에 대한 prediction을 하게 됨. 

## stationary and translation equivariance

시계열 분석에서는 시간에 상관 없이 데이터의 분포는 일정하다는 가정.
부분의 분포들은 모두 일정하다는 가정.
이미지 분석에서는 이미지의 각 부분에서의 분포는 일정하다는 가정이다. 따라서 사진의 각 patch에서 동일한 특징을 찾는 다는 점과 연결 됨. 

Translation equivariance: 입력이 바뀌면 출력가 바뀜. patch 내부에서는 traslation equivaraince 하다. 사진 전체에 대해 patch는 translation invariance해야 한다. 어느 영역에 있든지 동일한 기준으로 판단해서 동일한 결과가 나와야 한다. 

max pooling + stride + padding은 translation invariance을 구성하는 기준.

stride와 padding을 통해서 동일한 필터를 학습하는데 사진의 구석까지 학습시킬 수 있다. 

max pooling의 경우 커널의 값들 중 가장 큰 것만을 전달. 정보를 취합해서 가장 중요한 정보를 다음 레이어에 전달하는 것. 작은 patch들을 모아서 

convolution은 translation equivariance을 구성하는 기준? 아니면 모두 짬뽕?

translation invariance

locality



## convolution

신호를 증폭하거나 감소시킬 때 사용하는 연산.

https://www.deeplearningbook.org/contents/convnets.html
Suppose we are tracking the location of a spaceship with a laser sensor. Ourlaser sensor provides a single outputx(t), the position of the spaceship at timet.Bothxandtare real valued, that is, we can get a diﬀerent reading from the lasersensor at any instant in time.

Now suppose that our laser sensor is somewhat noisy. To obtain a less noisyestimate of the spaceship’s position, we would like to average several measurements. 노이즈가 있어서 평균을 내면 노이즈를 어느정도 무시할 수 있음. 경향성을 파악해서 자잘한 것들 제거. 

fig 9.1

현재 시점 t에서 x(t) 신호가 날아 옴. t부터 t-a 시간 만큼 가중 평균 해야 함. 따라서 t-a부터 t까지 x(a)에 대해서 가중치를 곱해줘서 더함 = 가중 평균.

n convolutional network terminology, the ﬁrst argument (in this example, thefunctionx) to the convolution is often referred to as theinput, and the second argument (in this example, the functionw) as thekernel. The output is sometimesreferred to as the feature map.

우주선 예시를 더 실제적으로 하면,
fig 9.3 

9.2 motivation
sparse interactions

Traditional neural network layers use matrix multiplication by a matrix ofparameters with a separate parameter describing the interaction between eachinput unit and each output unit

Convolutional networks, however, typically havesparseinteractions(also referred to assparse connectivityorsparse weights)

For example,when processing an image, the input image might have thousands or millions ofpixels, but we can detect small, meaningful features such as edges with kernelsthat occupy only tens or hundreds of pixels. This means that we need to storefewer parameters, which both reduces the memory requirements of the modeland improves its statistical eﬃciency.

fig 9.2

dense network는 1개의 변화에 대해 모든 다음 레이어가 영향을 받는다. sparse network는 연결되어 있는 뉴런들만 영향을 받음. 이미지 분석은 픽셀 크기에 비해서 한 부분만 해당하는 경향이 있다. 따라서 spare network을 사용할 수 있드면 더 좋다. spare network는 메모리도 적고 연산도 적게 한다. 할수만 있다면...! 인풋에서 특정 영역만 우리가 관심이 있는 부분이다. 이런 문제에서 dense network을 쓰면 관심이 없는 부분에서도 학습을 할 것. 학습시키려는 가중치가 오염될 것이다.  Thisis accomplished by making the kernel smaller than the input. kernel의 크기를 작게 한다 = 가중 평균하는 크기를 작게 함 -> 작은 부분들만 고려하고 나머지는 버린다. 



parameter sharing and 
Parameter sharingrefers to using the same parameter for more than onefunction in a model. In a traditional neural net, each element of the weight matrix is used exactly once when computing the output of a layer. 


As a synonym for parametersharing, one can say that a network hastied weights, because the value of theweight applied to one input is tied to the value of a weight applied elsewhere.



equivariant representations.

In the case of convolution, the particular form of parameter sharing causes thelayer to have a property calledequivarianceto translation. To say a function isequivariant means that if the input changes, the output changes in the same way.Speciﬁcally, a functionf(x) is equivariant to a functiongiff(g(x)) =g(f(x)). Inthe case of convolution, if we letgbe any function that translates the input, thatis, shifts it, then the convolution function is equivariant tog

For example, when processingimages, it is useful to detect edges in the ﬁrst layer of a convolutional network.The same edges appear more or less everywhere in the image, so it is practical toshare parameters across the entire image.


pooling이 나올 때 translation invariant에 대해 다시 살펴보기. 

---


특정한 가중치 함수(필터)를 적용하는 것. 
가중치가 multi variable인 경우가 일반적. 각 가중치 마다 더해서 더한다.

discrete case: 

continuous의 경우... x_0이 들어가면 실수 공간 R에 대해서 가중치만큼 곱해서 더하기 때문에 적분이다. 실수 공간 R에 mapping되는 g(R)이 가중치가 된다. 




d2l.ai 
fig 6.11

2d에 대한 일반적인 NN. input X의 각 i,j에 대해서 내적한 값이 1개의 뉴런에 도달한다. notation만 바꿈. i,j을 중심 위치라고 하고 , a와 b을 정수라고 하면, i,j을 중심으로 전체 영역에 대해서 내적하는 것으로 표현할 수 있음. 

Translation Invariance
fig 6.1.2

i,j에 상관 없이 같은 결과를 내야 한다. 따라서 가중치 V와 bias U는 i,j와 독럽직이어야 함. 기존에는 i,j에 하나의 V(i,j)가 매핑이었음. i,j마다 이 계산을 한번씩 하는 것이고 각 V가 필요했음. 

그런데 i,j에 상관 없어야 해서 전체 i,j에서 1개의 V을 사용함. i,j 위치 마다 같은 연산을 해야 하긴 하지만 1개의 V을 반복해서 사용 함. 기존의 V(i,j,a,b)에서 앞 두 차원이 사라지는 것.

fig 6.1.3
locality principle은 patch의 크기가 작다는 것. 판단 기준을 적은 범위에서 적용 한다. 그래서 a와 b에 제한을 둠. 이 영역 안에서만 내적한다. 이 식은 discrete에서의 convolutoin과 동일하다. 

실제 이미지는 3차원. (h, w, c)이다. 그래서 V도 3차원이 필요하다. 동일한 사물은 동일한 형태와 색을 가지고 있어서... 정보를 합쳐서 인식해야 함. 

6.1.7
색 color c과 다른 feature에 대해 학습할 channel에 대해서 동일하게 내적해서 hidden representation H으로 전달.

이게 convolutoin layer이다. 

color의 경우 모두 다 함. 색을 3종류의 채널로 표현. 그래서 색 별로 다 한다음에 정보를 합치기 때문에 summation. 그래서 H에 위치 정보 i,j와 채널 정보 d만 들어감. 

refetence 
https://d2l.ai/chapter_convolutional-neural-networks/why-conv.html
https://seoilgun.medium.com/cnn의-stationarity와-locality-610166700979

## cnn backpropagation

convolution을 미분하면, 미분 값으로 convolution 연산을 한 것과 같다.

$$
\begin{aligned}
    s(t) & =\int x(a) w(t-a) d a\\
    & = [x * w](t)\\
\end{aligned}\\
\begin{aligned}
    \frac{\partial}{\partial t}\int x(a) w(t-a) d a
    & = \int x(a) \frac{\partial}{\partial t} w(t-a) d a\\
    & = [x *w'](t)
\end{aligned}\\

\begin{aligned}
    \frac{\partial}{\partial w}\int x(a) w(t-a) d a
    & = \int x(a) \frac{\partial}{\partial w} w(t-a) d a\\
    & = x(t)
\end{aligned}\\
$$

chain rule으로 적용해봄. $J = h(s), s = [x*w](t)$이고, 중간의 미분항 $\frac{J}{s} = \delta$라고 하자. 
$$
    \frac{\partial J}{\partial t} = \frac{\partial J}{\partial s}\frac{\partial s}{\partial t}\\
    = \delta \cdot \frac{\partial s}{\partial t}\\
    = \delta \cdot [x*w'](t)\\
    \frac{\partial J}{\partial w} = \frac{\partial J}{\partial s} \frac{\partial s}{\partial w}\\
    = \delta \cdot x(t)
$$

CNN에서의 convolution 함수을 정의한다. `c`는 color, `i,j`는 input의 matrix index`(i,j)`. `n_c`는 channel 수. `V`는 input, `K`는 커널이다. $w$ 대신에 실수 가중치 matrix가 내적된다. 
$$
    Z_{j, k, n_c}=\sum_{ m, n, c} V_{j+m-1, k+n-1,c} K_{m, n, c, n_c}\\
$$

K의 각 채널은 1번씩 V와 convolution 된다. 하나의 채널에 대해서 미분하고 그 값을 stack 하면 전체 K에 대한 미분을 계산할 수 있음. 1개에 대해서 미분해봄. 가중치 함수가 실수 matrix 내적이다.  $[x*w](t) = <x,w>$. 따라서 X에 대해서 미분하면 `j,k`애 해당하는 커널의 $K_{j,k}$가 나온다.

K에 대해서 미분하면 각 `j,k` index의 값은 convolution 하기 전의 input 값이 그대로 나온다.

$$
    \frac{\partial}{\partial K_{\cdot,\cdot,\cdot, n_c}} Z_{j, k, n_c}=\sum_{ m, n, c} V_{j+m-1, k+n-1,c}\frac{\partial}{\partial K_{\cdot,\cdot,\cdot, n_c}}  K_{m, n, c, n_c}\\
    = V
$$

마찬가지로 V으로 미분하면 $K$가 나온다.

Z 이후의 레이어의 함수 값으로 loss 함수 J가 나왔다고 하자. 그러면 J에서 부터 Z까지의 미분 결과를 $\delta$라고 하면, 

$$
    \frac{\partial J}{\partial K} = \delta \cdot V\\
    \frac{\partial J}{\partial V} = \delta \cdot K

$$

# RNN
## back propagation
* data size = 1일때, 

$$
\partial_{w_{h}} h_{t}=\partial_{w_{h}} f\left(x_{t}, h_{t-1}, w_{h}\right)+\sum_{i=1}^{t-1}\left(\prod_{j=i+1}^{t} \partial_{h_{j-1}} f\left(x_{j}, h_{j-1}, w_{h}\right)\right) \partial_{w_{h}} f\left(x_{i}, h_{i-1}, w_{h}\right)
$$

w_h가 shared parameter으로 모든 시점에서 적용된다. 따라서 w_h으로 미분하면 모든 시점에 대해서 계산한 grad을 더해야 한다(전미분). 그래서 i=t-1의 시점에 적용된 grad는 $dh_t/df_{t-1} * df_{t-1}/dw$, i = 1의 시점에서 적용된 grad는 $dh_t/df_{t-1} * \cdots * df_{1}/dw$이다. 