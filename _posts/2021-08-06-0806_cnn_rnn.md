---
title: "0806_math"
last_modified_at: 2021-08-06T16:20:02-05:00
categories:
  - Blog
  - Camp
tags:
  - math
  - camp
---


## cnn backpropagation

convolution을 미분하면, 미분 값으로 convolution 연산을 한 것과 같다.

$$
\begin{aligned}
    s(t) & =\int x(a) w(t-a) d a\\
    & = [x * w](t)\\
\end{aligned}\\
\begin{aligned}
    \frac{\partial}{\partial t}\int x(a) w(t-a) d a
    & = \int x(a) \frac{\partial}{\partial t} w(t-a) d a\\
    & = [x *w'](t)
\end{aligned}\\

\begin{aligned}
    \frac{\partial}{\partial w}\int x(a) w(t-a) d a
    & = \int x(a) \frac{\partial}{\partial w} w(t-a) d a\\
    & = x(t)
\end{aligned}\\
$$

chain rule으로 적용해봄. $J = h(s), s = [x*w](t)$이고, 중간의 미분항 $\frac{J}{s} = \delta$라고 하자. 
$$
    \frac{\partial J}{\partial t} = \frac{\partial J}{\partial s}\frac{\partial s}{\partial t}\\
    = \delta \cdot \frac{\partial s}{\partial t}\\
    = \delta \cdot [x*w'](t)\\
    \frac{\partial J}{\partial w} = \frac{\partial J}{\partial s} \frac{\partial s}{\partial w}\\
    = \delta \cdot x(t)
$$

CNN에서의 convolution 함수을 정의한다. `c`는 color, `i,j`는 input의 matrix index`(i,j)`. `n_c`는 channel 수. `V`는 input, `K`는 커널이다. $w$ 대신에 실수 가중치 matrix가 내적된다. 
$$
    Z_{j, k, n_c}=\sum_{ m, n, c} V_{j+m-1, k+n-1,c} K_{m, n, c, n_c}\\
$$

K의 각 채널은 1번씩 V와 convolution 된다. 하나의 채널에 대해서 미분하고 그 값을 stack 하면 전체 K에 대한 미분을 계산할 수 있음. 1개에 대해서 미분해봄. 가중치 함수가 실수 matrix 내적이다.  $[x*w](t) = <x,w>$. 따라서 X에 대해서 미분하면 `j,k`애 해당하는 커널의 $K_{j,k}$가 나온다.

K에 대해서 미분하면 각 `j,k` index의 값은 convolution 하기 전의 input 값이 그대로 나온다.

$$
    \frac{\partial}{\partial K_{\cdot,\cdot,\cdot, n_c}} Z_{j, k, n_c}=\sum_{ m, n, c} V_{j+m-1, k+n-1,c}\frac{\partial}{\partial K_{\cdot,\cdot,\cdot, n_c}}  K_{m, n, c, n_c}\\
    = V
$$

마찬가지로 V으로 미분하면 $K$가 나온다.

Z 이후의 레이어의 함수 값으로 loss 함수 J가 나왔다고 하자. 그러면 J에서 부터 Z까지의 미분 결과를 $\delta$라고 하면, 

$$
    \frac{\partial J}{\partial K} = \delta \cdot V\\
    \frac{\partial J}{\partial V} = \delta \cdot K

$$

# RNN
## back propagation
* data size = 1일때, 

$$
\partial_{w_{h}} h_{t}=\partial_{w_{h}} f\left(x_{t}, h_{t-1}, w_{h}\right)+\sum_{i=1}^{t-1}\left(\prod_{j=i+1}^{t} \partial_{h_{j-1}} f\left(x_{j}, h_{j-1}, w_{h}\right)\right) \partial_{w_{h}} f\left(x_{i}, h_{i-1}, w_{h}\right)
$$

w_h가 shared parameter으로 모든 시점에서 적용된다. 따라서 w_h으로 미분하면 모든 시점에 대해서 계산한 grad을 더해야 한다(전미분). 그래서 i=t-1의 시점에 적용된 grad는 $dh_t/df_{t-1} * df_{t-1}/dw$, i = 1의 시점에서 적용된 grad는 $dh_t/df_{t-1} * \cdots * df_{1}/dw$이다. 